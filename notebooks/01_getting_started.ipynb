{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amharic Sentiment Analysis - Getting Started\n",
    "\n",
    "This notebook demonstrates how to use the Amharic Sentiment Analysis package for:\n",
    "1. Loading and preprocessing data\n",
    "2. Training different model architectures\n",
    "3. Evaluating model performance\n",
    "4. Making predictions on new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the package and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add project root to path (for development)\nimport sys\nsys.path.insert(0, '..')\n\n# Import modules\nfrom amharic_sentiment.data.dataset import AmharicSentimentDataset\nfrom amharic_sentiment.preprocessing.pipeline import PreprocessingPipeline\nfrom amharic_sentiment.models import CNN, BiLSTM, GRU, CNNBiLSTM\nfrom amharic_sentiment.training.trainer import Trainer\nfrom amharic_sentiment.evaluation.metrics import evaluate_model, get_classification_report\nfrom amharic_sentiment.evaluation.visualize import (\n    plot_training_history, \n    plot_confusion_matrix,\n    plot_metrics_comparison\n)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Modules imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Let's load the Amharic sentiment dataset and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = AmharicSentimentDataset(\n",
    "    max_words=15000,  # Maximum vocabulary size\n",
    "    max_len=20        # Maximum sequence length\n",
    ")\n",
    "\n",
    "# Load data from files\n",
    "dataset.load_from_files(\n",
    "    positive_file='../dataset/postive comment.txt',\n",
    "    negative_file='../dataset/negative comment.txt'\n",
    ")\n",
    "\n",
    "# Get dataset statistics\n",
    "stats = dataset.get_stats()\n",
    "print(\"Dataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some sample texts\n",
    "print(\"Sample positive texts:\")\n",
    "for i, (text, label) in enumerate(zip(dataset.texts[:3], dataset.labels[:3])):\n",
    "    print(f\"  {i+1}. {text[:60]}...\")\n",
    "\n",
    "print(\"\\nSample negative texts:\")\n",
    "negative_indices = [i for i, l in enumerate(dataset.labels) if l == 0][:3]\n",
    "for idx in negative_indices:\n",
    "    print(f\"  - {dataset.texts[idx][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "The preprocessing pipeline handles:\n",
    "- URL removal\n",
    "- Amharic punctuation cleaning\n",
    "- Character normalization (handling Unicode variants)\n",
    "- Labialized character normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing pipeline\n",
    "pipeline = PreprocessingPipeline()\n",
    "\n",
    "sample_text = \"ሰላም!! https://example.com ይህ ጥሩ ነው 123\"\n",
    "processed = pipeline.process(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Processed: {processed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Models\n",
    "\n",
    "Let's train the CNN-BiLSTM model (best performing architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer for CNN-BiLSTM\n",
    "trainer = Trainer(\n",
    "    model_type='cnn_bilstm',\n",
    "    dataset=dataset,\n",
    "    output_dir='../experiments',\n",
    "    experiment_name='cnn_bilstm_demo',\n",
    "    model_params={\n",
    "        'embedding_dim': 32,\n",
    "        'filters': 64,\n",
    "        'lstm_units': 64,\n",
    "        'dropout_rate_conv': 0.2,\n",
    "        'dropout_rate_lstm': 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data splits\n",
    "data_splits = trainer.prepare_data(\n",
    "    test_size=0.1,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(data_splits['y_train'])}\")\n",
    "print(f\"Validation samples: {len(data_splits['y_val'])}\")\n",
    "print(f\"Test samples: {len(data_splits['y_test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    epochs=5,  # Use more epochs for better results\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "Let's evaluate the trained model and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = trainer.evaluate(verbose=1)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Accuracy:  {results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {results['f1_score']:.4f}\")\n",
    "print(f\"  ROC AUC:   {results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history, metrics=['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    results['confusion_matrix'],\n",
    "    labels=['Negative', 'Positive']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making Predictions\n",
    "\n",
    "Let's use the trained model to predict sentiment on new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts for prediction\n",
    "test_texts = [\n",
    "    \"ጥሩ ስራ ነው ተባረኩ\",           # Positive: Good work, be blessed\n",
    "    \"በጣም መጥፎ ውሳኔ ነው\",           # Negative: Very bad decision\n",
    "    \"እግዚአብሔር ይስጥልን\",          # Positive: May God give us\n",
    "    \"ሀገር አፍራሽ\",                  # Negative: Country destroyer\n",
    "]\n",
    "\n",
    "# Preprocess and tokenize\n",
    "processed_texts = [pipeline.process(text) for text in test_texts]\n",
    "sequences = dataset.tokenizer.texts_to_sequences(processed_texts)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(sequences, maxlen=20, padding='post')\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.model.predict_proba(padded)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "for text, prob in zip(test_texts, predictions):\n",
    "    sentiment = \"Positive\" if prob > 0.5 else \"Negative\"\n",
    "    confidence = prob if prob > 0.5 else 1 - prob\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Sentiment: {sentiment} (confidence: {confidence:.2%})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Models\n",
    "\n",
    "Let's train and compare different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison (simplified for demo)\n",
    "model_types = ['cnn', 'bilstm', 'gru', 'cnn_bilstm']\n",
    "comparison_results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\nTraining {model_type.upper()}...\")\n",
    "    \n",
    "    # Create new dataset instance\n",
    "    ds = AmharicSentimentDataset(max_words=15000, max_len=20)\n",
    "    ds.load_from_files(\n",
    "        '../dataset/postive comment.txt',\n",
    "        '../dataset/negative comment.txt'\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    t = Trainer(\n",
    "        model_type=model_type,\n",
    "        dataset=ds,\n",
    "        output_dir='../experiments/comparison',\n",
    "        experiment_name=f'{model_type}_comparison'\n",
    "    )\n",
    "    \n",
    "    # Train (fewer epochs for demo)\n",
    "    t.train(epochs=3, batch_size=64, verbose=0)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = t.evaluate(verbose=0)\n",
    "    comparison_results[model_type] = {\n",
    "        'accuracy': results['accuracy'],\n",
    "        'precision': results['precision'],\n",
    "        'recall': results['recall'],\n",
    "        'f1_score': results['f1_score']\n",
    "    }\n",
    "    print(f\"  Accuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plot_metrics_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading Models\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = trainer.save_model()\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_path = str(trainer.output_dir / 'tokenizer.pkl')\n",
    "dataset.save_tokenizer(tokenizer_path)\n",
    "print(f\"Tokenizer saved to: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Load tokenizer\n",
    "with open(tokenizer_path, 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Data Loading**: Loading Amharic text data from files\n",
    "2. **Preprocessing**: Cleaning and normalizing Amharic text\n",
    "3. **Model Training**: Training CNN-BiLSTM and other architectures\n",
    "4. **Evaluation**: Measuring accuracy, precision, recall, F1 score\n",
    "5. **Prediction**: Classifying new text samples\n",
    "6. **Model Comparison**: Comparing different architectures\n",
    "7. **Model Persistence**: Saving and loading trained models\n",
    "\n",
    "For more advanced usage, see:\n",
    "- `configs/` for configuration options\n",
    "- `scripts/train.py` for command-line training\n",
    "- API documentation in the source code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}