# Transformer Model Configuration
# ================================
# Fine-tuning pre-trained multilingual models

data:
  positive_file: "dataset/postive comment.txt"
  negative_file: "dataset/negative comment.txt"
  max_length: 128  # Transformer max sequence length
  test_size: 0.1
  val_size: 0.1
  random_state: 42

model:
  # Options: mbert, xlm-roberta, xlm-roberta-large, distilbert-multi, afro-xlmr
  model_name: "xlm-roberta"
  num_labels: 1  # Binary classification
  dropout: 0.1
  freeze_base: false
  freeze_layers: null  # Number of bottom layers to freeze (e.g., 6)

training:
  epochs: 5
  batch_size: 16  # Smaller batch for transformers
  learning_rate: 2e-5  # Lower LR for fine-tuning
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  scheduler_type: "linear"  # linear, cosine
  early_stopping_patience: 3

output_dir: "experiments/transformer"
